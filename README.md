Adversarial Attacks on MNIST using FGSM
This project demonstrates a basic image classification pipeline on the MNIST dataset using a simple MLP (multi-layer perceptron) classifier, and then applies the Fast Gradient Sign Method (FGSM) to perform adversarial attacks.

ğŸ§  What It Does
Loads and visualizes the MNIST dataset (handwritten digits)

Builds a neural network to classify digits 0â€“9

Achieves over 94% accuracy on clean test data

Applies FGSM to generate adversarial examples

Shows that small, imperceptible perturbations can cause the model to misclassify

Measures accuracy degradation under attack

âš”ï¸ Adversarial Technique
FGSM (Fast Gradient Sign Method):
Adds perturbation in the direction of the gradient of the loss with respect to input pixels.
The result is a nearly identical image that fools the classifier.

ğŸ“Š Results
Test Scenario: Clean Images â†’ Accuracy: ~94.65%
Test Scenario: Adversarial (Îµ = 0.2) â†’ Accuracy: ~XX% (to be filled based on your run)

ğŸ–¼ï¸ Example (Adversarial Confusion)
3 â†’ 8
1 â†’ 7
2 â†’ 6

ğŸ“ Files
main.ipynb â€“ Notebook with full pipeline and attack

requirements.txt â€“ Python packages used

.venv/ â€“ (not included in GitHub, local environment)

âœ… How to Run
pip install -r requirements.txt
jupyter notebook main.ipynb

Tested on macOS with Python 3.10+ and PyTorch.

ğŸ” Author
Emanuele Rossi â€“ AI Red Teaming project portfolio.

